{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Sentiment Analysis using News Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Data.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>A 'hindrance to operations': extracts from the...</td>\n",
       "      <td>Scorecard</td>\n",
       "      <td>Hughes' instant hit buoys Blues</td>\n",
       "      <td>Jack gets his skates on at ice-cold Alex</td>\n",
       "      <td>Chaos as Maracana builds up for United</td>\n",
       "      <td>Depleted Leicester prevail as Elliott spoils E...</td>\n",
       "      <td>Hungry Spurs sense rich pickings</td>\n",
       "      <td>Gunners so wide of an easy target</td>\n",
       "      <td>...</td>\n",
       "      <td>Flintoff injury piles on woe for England</td>\n",
       "      <td>Hunters threaten Jospin with new battle of the...</td>\n",
       "      <td>Kohl's successor drawn into scandal</td>\n",
       "      <td>The difference between men and women</td>\n",
       "      <td>Sara Denver, nurse turned solicitor</td>\n",
       "      <td>Diana's landmine crusade put Tories in a panic</td>\n",
       "      <td>Yeltsin's resignation caught opposition flat-f...</td>\n",
       "      <td>Russian roulette</td>\n",
       "      <td>Sold out</td>\n",
       "      <td>Recovering a title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>Scorecard</td>\n",
       "      <td>The best lake scene</td>\n",
       "      <td>Leader: German sleaze inquiry</td>\n",
       "      <td>Cheerio, boyo</td>\n",
       "      <td>The main recommendations</td>\n",
       "      <td>Has Cubie killed fees?</td>\n",
       "      <td>Has Cubie killed fees?</td>\n",
       "      <td>Has Cubie killed fees?</td>\n",
       "      <td>...</td>\n",
       "      <td>On the critical list</td>\n",
       "      <td>The timing of their lives</td>\n",
       "      <td>Dear doctor</td>\n",
       "      <td>Irish court halts IRA man's extradition to Nor...</td>\n",
       "      <td>Burundi peace initiative fades after rebels re...</td>\n",
       "      <td>PE points the way forward to the ECB</td>\n",
       "      <td>Campaigners keep up pressure on Nazi war crime...</td>\n",
       "      <td>Jane Ratcliffe</td>\n",
       "      <td>Yet more things you wouldn't know without the ...</td>\n",
       "      <td>Millennium bug fails to bite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>0</td>\n",
       "      <td>Coventry caught on counter by Flo</td>\n",
       "      <td>United's rivals on the road to Rio</td>\n",
       "      <td>Thatcher issues defence before trial by video</td>\n",
       "      <td>Police help Smith lay down the law at Everton</td>\n",
       "      <td>Tale of Trautmann bears two more retellings</td>\n",
       "      <td>England on the rack</td>\n",
       "      <td>Pakistan retaliate with call for video of Walsh</td>\n",
       "      <td>Cullinan continues his Cape monopoly</td>\n",
       "      <td>...</td>\n",
       "      <td>South Melbourne (Australia)</td>\n",
       "      <td>Necaxa (Mexico)</td>\n",
       "      <td>Real Madrid (Spain)</td>\n",
       "      <td>Raja Casablanca (Morocco)</td>\n",
       "      <td>Corinthians (Brazil)</td>\n",
       "      <td>Tony's pet project</td>\n",
       "      <td>Al Nassr (Saudi Arabia)</td>\n",
       "      <td>Ideal Holmes show</td>\n",
       "      <td>Pinochet leaves hospital after tests</td>\n",
       "      <td>Useful links</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilgrim knows how to progress</td>\n",
       "      <td>Thatcher facing ban</td>\n",
       "      <td>McIlroy calls for Irish fighting spirit</td>\n",
       "      <td>Leicester bin stadium blueprint</td>\n",
       "      <td>United braced for Mexican wave</td>\n",
       "      <td>Auntie back in fashion, even if the dress look...</td>\n",
       "      <td>Shoaib appeal goes to the top</td>\n",
       "      <td>Hussain hurt by 'shambles' but lays blame on e...</td>\n",
       "      <td>...</td>\n",
       "      <td>Putin admits Yeltsin quit to give him a head s...</td>\n",
       "      <td>BBC worst hit as digital TV begins to bite</td>\n",
       "      <td>How much can you pay for...</td>\n",
       "      <td>Christmas glitches</td>\n",
       "      <td>Upending a table, Chopping a line and Scoring ...</td>\n",
       "      <td>Scientific evidence 'unreliable', defence claims</td>\n",
       "      <td>Fusco wins judicial review in extradition case</td>\n",
       "      <td>Rebels thwart Russian advance</td>\n",
       "      <td>Blair orders shake-up of failing NHS</td>\n",
       "      <td>Lessons of law's hard heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>1</td>\n",
       "      <td>Hitches and Horlocks</td>\n",
       "      <td>Beckham off but United survive</td>\n",
       "      <td>Breast cancer screening</td>\n",
       "      <td>Alan Parker</td>\n",
       "      <td>Guardian readers: are you all whingers?</td>\n",
       "      <td>Hollywood Beyond</td>\n",
       "      <td>Ashes and diamonds</td>\n",
       "      <td>Whingers - a formidable minority</td>\n",
       "      <td>...</td>\n",
       "      <td>Most everywhere:  UDIs</td>\n",
       "      <td>Most wanted:  Chloe lunettes</td>\n",
       "      <td>Return of the cane 'completely off the agenda'</td>\n",
       "      <td>From Sleepy Hollow to Greeneland</td>\n",
       "      <td>Blunkett outlines vision for over 11s</td>\n",
       "      <td>Embattled Dobson attacks 'play now, pay later'...</td>\n",
       "      <td>Doom and the Dome</td>\n",
       "      <td>What is the north-south divide?</td>\n",
       "      <td>Aitken released from jail</td>\n",
       "      <td>Gone aloft</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Label                                               Top1  \\\n",
       "0  2000-01-03      0  A 'hindrance to operations': extracts from the...   \n",
       "1  2000-01-04      0                                          Scorecard   \n",
       "2  2000-01-05      0                  Coventry caught on counter by Flo   \n",
       "3  2000-01-06      1                      Pilgrim knows how to progress   \n",
       "4  2000-01-07      1                               Hitches and Horlocks   \n",
       "\n",
       "                                 Top2  \\\n",
       "0                           Scorecard   \n",
       "1                 The best lake scene   \n",
       "2  United's rivals on the road to Rio   \n",
       "3                 Thatcher facing ban   \n",
       "4      Beckham off but United survive   \n",
       "\n",
       "                                            Top3  \\\n",
       "0                Hughes' instant hit buoys Blues   \n",
       "1                  Leader: German sleaze inquiry   \n",
       "2  Thatcher issues defence before trial by video   \n",
       "3        McIlroy calls for Irish fighting spirit   \n",
       "4                        Breast cancer screening   \n",
       "\n",
       "                                            Top4  \\\n",
       "0       Jack gets his skates on at ice-cold Alex   \n",
       "1                                  Cheerio, boyo   \n",
       "2  Police help Smith lay down the law at Everton   \n",
       "3                Leicester bin stadium blueprint   \n",
       "4                                    Alan Parker   \n",
       "\n",
       "                                          Top5  \\\n",
       "0       Chaos as Maracana builds up for United   \n",
       "1                     The main recommendations   \n",
       "2  Tale of Trautmann bears two more retellings   \n",
       "3               United braced for Mexican wave   \n",
       "4      Guardian readers: are you all whingers?   \n",
       "\n",
       "                                                Top6  \\\n",
       "0  Depleted Leicester prevail as Elliott spoils E...   \n",
       "1                             Has Cubie killed fees?   \n",
       "2                                England on the rack   \n",
       "3  Auntie back in fashion, even if the dress look...   \n",
       "4                                   Hollywood Beyond   \n",
       "\n",
       "                                              Top7  \\\n",
       "0                 Hungry Spurs sense rich pickings   \n",
       "1                           Has Cubie killed fees?   \n",
       "2  Pakistan retaliate with call for video of Walsh   \n",
       "3                    Shoaib appeal goes to the top   \n",
       "4                               Ashes and diamonds   \n",
       "\n",
       "                                                Top8  ...  \\\n",
       "0                  Gunners so wide of an easy target  ...   \n",
       "1                             Has Cubie killed fees?  ...   \n",
       "2               Cullinan continues his Cape monopoly  ...   \n",
       "3  Hussain hurt by 'shambles' but lays blame on e...  ...   \n",
       "4                   Whingers - a formidable minority  ...   \n",
       "\n",
       "                                               Top16  \\\n",
       "0           Flintoff injury piles on woe for England   \n",
       "1                               On the critical list   \n",
       "2                        South Melbourne (Australia)   \n",
       "3  Putin admits Yeltsin quit to give him a head s...   \n",
       "4                             Most everywhere:  UDIs   \n",
       "\n",
       "                                               Top17  \\\n",
       "0  Hunters threaten Jospin with new battle of the...   \n",
       "1                          The timing of their lives   \n",
       "2                                    Necaxa (Mexico)   \n",
       "3         BBC worst hit as digital TV begins to bite   \n",
       "4                       Most wanted:  Chloe lunettes   \n",
       "\n",
       "                                            Top18  \\\n",
       "0             Kohl's successor drawn into scandal   \n",
       "1                                     Dear doctor   \n",
       "2                             Real Madrid (Spain)   \n",
       "3                     How much can you pay for...   \n",
       "4  Return of the cane 'completely off the agenda'   \n",
       "\n",
       "                                               Top19  \\\n",
       "0               The difference between men and women   \n",
       "1  Irish court halts IRA man's extradition to Nor...   \n",
       "2                          Raja Casablanca (Morocco)   \n",
       "3                                 Christmas glitches   \n",
       "4                   From Sleepy Hollow to Greeneland   \n",
       "\n",
       "                                               Top20  \\\n",
       "0                Sara Denver, nurse turned solicitor   \n",
       "1  Burundi peace initiative fades after rebels re...   \n",
       "2                               Corinthians (Brazil)   \n",
       "3  Upending a table, Chopping a line and Scoring ...   \n",
       "4              Blunkett outlines vision for over 11s   \n",
       "\n",
       "                                               Top21  \\\n",
       "0     Diana's landmine crusade put Tories in a panic   \n",
       "1               PE points the way forward to the ECB   \n",
       "2                                 Tony's pet project   \n",
       "3   Scientific evidence 'unreliable', defence claims   \n",
       "4  Embattled Dobson attacks 'play now, pay later'...   \n",
       "\n",
       "                                               Top22  \\\n",
       "0  Yeltsin's resignation caught opposition flat-f...   \n",
       "1  Campaigners keep up pressure on Nazi war crime...   \n",
       "2                            Al Nassr (Saudi Arabia)   \n",
       "3     Fusco wins judicial review in extradition case   \n",
       "4                                  Doom and the Dome   \n",
       "\n",
       "                             Top23  \\\n",
       "0                 Russian roulette   \n",
       "1                   Jane Ratcliffe   \n",
       "2                Ideal Holmes show   \n",
       "3    Rebels thwart Russian advance   \n",
       "4  What is the north-south divide?   \n",
       "\n",
       "                                               Top24  \\\n",
       "0                                           Sold out   \n",
       "1  Yet more things you wouldn't know without the ...   \n",
       "2               Pinochet leaves hospital after tests   \n",
       "3               Blair orders shake-up of failing NHS   \n",
       "4                          Aitken released from jail   \n",
       "\n",
       "                          Top25  \n",
       "0            Recovering a title  \n",
       "1  Millennium bug fails to bite  \n",
       "2                  Useful links  \n",
       "3   Lessons of law's hard heart  \n",
       "4                    Gone aloft  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2000-01-03', '2000-01-04', '2000-01-05', ..., '2016-06-29',\n",
       "       '2016-06-30', '2016-07-01'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4101"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['Date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['Date'] > '20140101'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['Date'] <= '20150101']\n",
    "test = df[df['Date'] > '20140101']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Label', 'Top1', 'Top2', 'Top3', 'Top4', 'Top5', 'Top6', 'Top7',\n",
       "       'Top8', 'Top9', 'Top10', 'Top11', 'Top12', 'Top13', 'Top14', 'Top15',\n",
       "       'Top16', 'Top17', 'Top18', 'Top19', 'Top20', 'Top21', 'Top22', 'Top23',\n",
       "       'Top24', 'Top25'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train.drop(['Date', 'Label'], axis=1)\n",
    "#remove characters other than chars and digits[it will work when it is in english]\n",
    "data.replace(\"[^a-zA-Z0-9]\",\" \",regex=True, inplace=True)\n",
    "\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a  hindrance to operations   extracts from the leaked reports scorecard hughes  instant hit buoys blues jack gets his skates on at ice cold alex chaos as maracana builds up for united depleted leicester prevail as elliott spoils everton s party hungry spurs sense rich pickings gunners so wide of an easy target derby raise a glass to strupar s debut double southgate strikes  leeds pay the penalty hammers hand robson a youthful lesson saints party like it s 1999 wear wolves have turned into lambs stump mike catches testy gough s taunt langer escapes to hit 167 flintoff injury piles on woe for england hunters threaten jospin with new battle of the somme kohl s successor drawn into scandal the difference between men and women sara denver  nurse turned solicitor diana s landmine crusade put tories in a panic yeltsin s resignation caught opposition flat footed russian roulette sold out recovering a title'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines = [' '.join(ent) for ent in data.astype(str).to_numpy()]\n",
    "headlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3975x598244 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1093034 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## BAG OF WORDS\n",
    "countvector=CountVectorizer(ngram_range=(2,2))\n",
    "traindataset=countvector.fit_transform(headlines)\n",
    "traindataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 245398)\t1\n",
      "  (0, 535245)\t1\n",
      "  (0, 380066)\t1\n",
      "  (0, 183750)\t1\n",
      "  (0, 213096)\t1\n",
      "  (0, 519728)\t1\n",
      "  (0, 304796)\t1\n",
      "  (0, 438837)\t1\n",
      "  (0, 461434)\t1\n",
      "  (0, 253195)\t1\n",
      "  (0, 268324)\t1\n",
      "  (0, 247159)\t1\n",
      "  (0, 90053)\t1\n",
      "  (0, 78868)\t1\n",
      "  (0, 284275)\t1\n",
      "  (0, 221118)\t1\n",
      "  (0, 246540)\t1\n",
      "  (0, 477528)\t1\n",
      "  (0, 374644)\t1\n",
      "  (0, 53402)\t1\n",
      "  (0, 254966)\t1\n",
      "  (0, 119667)\t1\n",
      "  (0, 26262)\t1\n",
      "  (0, 107315)\t1\n",
      "  (0, 49633)\t1\n",
      "  :\t:\n",
      "  (0, 38920)\t1\n",
      "  (0, 588915)\t1\n",
      "  (0, 456060)\t1\n",
      "  (0, 149550)\t1\n",
      "  (0, 361584)\t1\n",
      "  (0, 548005)\t1\n",
      "  (0, 482053)\t1\n",
      "  (0, 152687)\t1\n",
      "  (0, 299546)\t1\n",
      "  (0, 137249)\t1\n",
      "  (0, 422496)\t1\n",
      "  (0, 539626)\t1\n",
      "  (0, 261458)\t1\n",
      "  (0, 391019)\t1\n",
      "  (0, 594964)\t1\n",
      "  (0, 440178)\t1\n",
      "  (0, 103416)\t1\n",
      "  (0, 380491)\t1\n",
      "  (0, 197354)\t1\n",
      "  (0, 200392)\t1\n",
      "  (0, 453009)\t1\n",
      "  (0, 449446)\t1\n",
      "  (0, 481642)\t1\n",
      "  (0, 384652)\t1\n",
      "  (0, 432622)\t1\n"
     ]
    }
   ],
   "source": [
    "for i in traindataset:\n",
    "    break\n",
    "for j in i:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=150, min_samples_leaf=2,\n",
       "                       min_samples_split=4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement RandomForest Classifier\n",
    "randomclassifier=RandomForestClassifier(n_estimators=100,criterion='entropy', max_depth=150, min_samples_split=4, min_samples_leaf=2)\n",
    "randomclassifier.fit(traindataset,train['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': 150,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 4,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomclassifier.get_params(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict \n",
    "\n",
    "test_transform = [' '.join(ent) for ent in test.iloc[:,2:27].astype(str).to_numpy()]\n",
    "test_dataset = countvector.transform(test_transform)\n",
    "predictions = randomclassifier.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import library to check accuracy\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1538  341]\n",
      " [   1 2095]]\n",
      "0.9139622641509434\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90      1879\n",
      "           1       0.86      1.00      0.92      2096\n",
      "\n",
      "    accuracy                           0.91      3975\n",
      "   macro avg       0.93      0.91      0.91      3975\n",
      "weighted avg       0.93      0.91      0.91      3975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_pred = randomclassifier.predict(traindataset)\n",
    "matrix=confusion_matrix(train['Label'],t_pred)\n",
    "print(matrix)\n",
    "score=accuracy_score(train['Label'],t_pred)\n",
    "print(score)\n",
    "report=classification_report(train['Label'],t_pred)\n",
    "print(report)\n",
    "##### train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3723    1\n",
       "3724    0\n",
       "3725    0\n",
       "3726    1\n",
       "3727    1\n",
       "       ..\n",
       "4096    0\n",
       "4097    1\n",
       "4098    1\n",
       "4099    1\n",
       "4100    1\n",
       "Name: Label, Length: 378, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[147  39]\n",
      " [ 10 182]]\n",
      "0.8703703703703703\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.79      0.86       186\n",
      "           1       0.82      0.95      0.88       192\n",
      "\n",
      "    accuracy                           0.87       378\n",
      "   macro avg       0.88      0.87      0.87       378\n",
      "weighted avg       0.88      0.87      0.87       378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix=confusion_matrix(test['Label'],predictions)\n",
    "print(matrix)\n",
    "score=accuracy_score(test['Label'],predictions)\n",
    "print(score)\n",
    "report=classification_report(test['Label'],predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 100\n",
    "model = Word2Vec(sentences=[i.split() for i in headlines], size=num_features, min_count=1)#num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veere\\.conda\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.6807256 , -0.24691877,  0.07629899, -0.00871047,  0.08631532,\n",
       "        0.10048051, -0.5207038 , -0.70714396,  1.1267912 ,  0.43294978,\n",
       "       -0.3930759 , -0.9767484 , -0.0777116 ,  0.22743252,  0.4520202 ,\n",
       "        0.56377333,  0.12350605, -0.48788184, -0.12702146, -0.78994644,\n",
       "       -0.7346795 , -0.9460096 ,  0.3626299 ,  0.44354877, -0.6701896 ,\n",
       "        1.1578509 ,  0.49880397, -1.0119852 , -0.8802886 , -0.27076665,\n",
       "       -0.05876817,  0.2680183 , -0.270454  , -0.8645132 ,  0.18421128,\n",
       "        0.55647147, -0.1810478 ,  0.6770108 ,  0.63187194,  0.0320318 ,\n",
       "       -0.34063798, -0.4223452 ,  0.4565597 , -0.3240429 ,  0.51623636,\n",
       "        0.82047343, -0.45232937, -0.00850398,  0.8592557 , -0.81392026,\n",
       "       -0.75316817, -0.7649525 ,  0.02638813,  0.38127822,  0.18333866,\n",
       "        0.5878552 , -0.5450619 , -0.4061632 ,  0.9068486 ,  0.2343175 ,\n",
       "       -0.5429593 ,  0.3094613 , -0.02988186, -0.11252353, -0.9316999 ,\n",
       "       -0.18031469,  0.47067913,  0.22751282,  0.34561345,  0.17846891,\n",
       "        0.36680835,  0.8996167 , -0.35186982, -0.01681921,  0.638131  ,\n",
       "        0.35748583,  0.39947882, -0.786196  ,  0.01925336,  0.7659387 ,\n",
       "        0.05933532,  0.06974374, -0.90269154, -0.9399989 , -0.89446825,\n",
       "        0.95751405,  0.868895  ,  0.8323001 ,  0.6387902 , -0.75638443,\n",
       "       -0.8250984 , -0.43854335,  0.47381422, -0.02401711,  0.13836288,\n",
       "        0.19711699,  0.55164045, -0.30591017,  1.3303117 , -0.9307719 ,\n",
       "        0.66067326, -0.2638881 ,  0.2810786 ,  0.43541598, -0.6918974 ,\n",
       "       -0.491832  , -0.14286728,  0.03626606,  0.8917722 ,  0.9626431 ,\n",
       "       -0.6507219 , -0.61813617, -0.54870677, -0.3614823 , -0.17173894,\n",
       "       -0.5168427 ,  0.22598228,  0.83189183,  0.7101754 ,  1.3041451 ,\n",
       "        0.4504536 , -0.09999249,  0.979022  ,  0.14770621,  1.0060471 ,\n",
       "       -0.1945157 , -0.24595846,  1.0410659 ,  0.2245747 , -0.92810094,\n",
       "       -0.25017536,  0.40425557, -0.45251018,  0.14156467,  0.18754935,\n",
       "       -0.4138582 ,  0.403698  , -0.81998944,  0.67159   , -0.7961294 ,\n",
       "        0.65326494,  0.68813753, -0.13371971,  0.52797776, -0.78461844,\n",
       "        0.81951696, -0.7853947 ,  0.15015647,  0.27562502,  0.3366724 ,\n",
       "        0.80371076,  0.30631417, -0.7073971 ,  0.16792631,  0.04490067,\n",
       "       -0.97438776,  0.31560317,  0.4904977 ,  0.47105557,  0.39262262,\n",
       "       -0.9100076 ,  0.00505968, -0.93277085, -0.51241505, -0.19965833,\n",
       "       -1.2909406 ,  0.24368954,  0.64160377,  0.04204729,  0.36015633,\n",
       "       -1.0128212 ,  1.4450278 , -1.0394008 ,  0.0735136 ,  0.54037213,\n",
       "       -0.6652777 ,  0.04361727, -0.41232565, -0.75850093, -0.20420843,\n",
       "       -0.37875286, -0.50431794,  1.497905  , -0.53964585,  0.08345823,\n",
       "       -0.15558404,  0.23754917,  0.33694947, -0.06192311, -0.14221878,\n",
       "       -0.15325631,  0.65829885,  0.6702506 ,  0.42452508,  0.06438718,\n",
       "        0.3372726 , -0.14968926,  0.39891276, -0.5657253 , -0.2992138 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veere\\.conda\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "w2v = dict(zip(model.wv.index2word , model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'to',\n",
       " 'of',\n",
       " 'in',\n",
       " 'a',\n",
       " 'and',\n",
       " 's',\n",
       " 'for',\n",
       " 'b',\n",
       " 'on',\n",
       " 'is',\n",
       " 'by',\n",
       " 'with',\n",
       " 'as',\n",
       " 'that',\n",
       " 'from',\n",
       " 'has',\n",
       " 'at',\n",
       " 'it',\n",
       " 'are',\n",
       " 'be',\n",
       " 'us',\n",
       " 'over',\n",
       " 'new',\n",
       " 'have',\n",
       " 'after',\n",
       " 'an',\n",
       " 'up',\n",
       " 'says',\n",
       " 'not',\n",
       " 'world',\n",
       " 'will',\n",
       " 'was',\n",
       " 'out',\n",
       " '1',\n",
       " 'who',\n",
       " 'its',\n",
       " 'his',\n",
       " 'their',\n",
       " 'israel',\n",
       " 'government',\n",
       " 'police',\n",
       " 'china',\n",
       " 'they',\n",
       " 'more',\n",
       " 'no',\n",
       " 'people',\n",
       " 'war',\n",
       " 'against',\n",
       " 'year',\n",
       " 'into',\n",
       " 'u',\n",
       " 'been',\n",
       " 'one',\n",
       " 'russia',\n",
       " '000',\n",
       " 'uk',\n",
       " 'all',\n",
       " 'this',\n",
       " '2',\n",
       " 'than',\n",
       " 'years',\n",
       " 'he',\n",
       " 'first',\n",
       " 'i',\n",
       " 'we',\n",
       " 't',\n",
       " 'but',\n",
       " 'about',\n",
       " 'you',\n",
       " 'review',\n",
       " 'president',\n",
       " 'killed',\n",
       " 'iran',\n",
       " 'two',\n",
       " 'man',\n",
       " 'country',\n",
       " 'off',\n",
       " 'israeli',\n",
       " 'news',\n",
       " 'what',\n",
       " 'may',\n",
       " 'north',\n",
       " 'now',\n",
       " 'can',\n",
       " 'state',\n",
       " 'russian',\n",
       " 'how',\n",
       " 'were',\n",
       " 'if',\n",
       " 'court',\n",
       " 'or',\n",
       " 'leader',\n",
       " 'korea',\n",
       " 'said',\n",
       " 'down',\n",
       " '0',\n",
       " 'could',\n",
       " 'back',\n",
       " 'minister',\n",
       " 'death',\n",
       " '3',\n",
       " 'time',\n",
       " 'united',\n",
       " 'british',\n",
       " 'old',\n",
       " 'military',\n",
       " 'city',\n",
       " 'south',\n",
       " 'attack',\n",
       " 'england',\n",
       " 'being',\n",
       " 'un',\n",
       " 'nuclear',\n",
       " 'report',\n",
       " 'london',\n",
       " 'day',\n",
       " 'women',\n",
       " 'say',\n",
       " 'football',\n",
       " 'iraq',\n",
       " 'found',\n",
       " 'india',\n",
       " 'chinese',\n",
       " 'law',\n",
       " 'gaza',\n",
       " 'amp',\n",
       " 'children',\n",
       " 'oil',\n",
       " 'rights',\n",
       " 'syria',\n",
       " 'last',\n",
       " 'anti',\n",
       " 'top',\n",
       " 'her',\n",
       " 'would',\n",
       " 'under',\n",
       " 'bank',\n",
       " 'them',\n",
       " 'most',\n",
       " 'cup',\n",
       " 'just',\n",
       " 'life',\n",
       " 'million',\n",
       " 'dead',\n",
       " 'human',\n",
       " 'pakistan',\n",
       " 'forces',\n",
       " 'ban',\n",
       " 'get',\n",
       " 'japan',\n",
       " 'only',\n",
       " 'three',\n",
       " '10',\n",
       " 'bbc',\n",
       " 'west',\n",
       " 'germany',\n",
       " 'france',\n",
       " 'should',\n",
       " 'eu',\n",
       " 'had',\n",
       " 'french',\n",
       " 'which',\n",
       " 'europe',\n",
       " 'why',\n",
       " 'set',\n",
       " 'so',\n",
       " 'drug',\n",
       " 'home',\n",
       " 'face',\n",
       " 'internet',\n",
       " 'calls',\n",
       " 'letters',\n",
       " 'britain',\n",
       " 'there',\n",
       " 'power',\n",
       " 'deal',\n",
       " 'end',\n",
       " 'right',\n",
       " 'international',\n",
       " 'when',\n",
       " 'take',\n",
       " 'go',\n",
       " 'week',\n",
       " 'al',\n",
       " 'other',\n",
       " 'like',\n",
       " '5',\n",
       " 'make',\n",
       " 'saudi',\n",
       " 'ukraine',\n",
       " '4',\n",
       " 'german',\n",
       " 'former',\n",
       " 'european',\n",
       " 'do',\n",
       " 'group',\n",
       " 'security',\n",
       " 'high',\n",
       " 'public',\n",
       " 'canada',\n",
       " 'woman',\n",
       " 'protest',\n",
       " 'union',\n",
       " 'palestinian',\n",
       " 'fire',\n",
       " 'crisis',\n",
       " 'sex',\n",
       " 'army',\n",
       " 'cd',\n",
       " 'egypt',\n",
       " 'global',\n",
       " 'use',\n",
       " 'chief',\n",
       " 'video',\n",
       " 'pay',\n",
       " 'our',\n",
       " 'my',\n",
       " 'arrested',\n",
       " 'big',\n",
       " 'media',\n",
       " 'australia',\n",
       " 'party',\n",
       " 'plans',\n",
       " 'league',\n",
       " 'men',\n",
       " 'n',\n",
       " 'afghanistan',\n",
       " 'american',\n",
       " 'wikileaks',\n",
       " 'help',\n",
       " 'officials',\n",
       " 'syrian',\n",
       " 'must',\n",
       " 'some',\n",
       " 'isis',\n",
       " 'your',\n",
       " 'attacks',\n",
       " 'protesters',\n",
       " 'claims',\n",
       " 'troops',\n",
       " 'obituary',\n",
       " 'australian',\n",
       " 'mexico',\n",
       " 'way',\n",
       " 'water',\n",
       " 'billion',\n",
       " 'free',\n",
       " 'protests',\n",
       " 'case',\n",
       " 'hit',\n",
       " 'obama',\n",
       " 'because',\n",
       " 'prime',\n",
       " 'call',\n",
       " 'turkey',\n",
       " 'thousands',\n",
       " 'before',\n",
       " 'putin',\n",
       " 'strike',\n",
       " 'blair',\n",
       " 'bill',\n",
       " 'islamic',\n",
       " 'still',\n",
       " 'air',\n",
       " 'rugby',\n",
       " 'child',\n",
       " 'school',\n",
       " 'five',\n",
       " 'plan',\n",
       " 'cricket',\n",
       " 'africa',\n",
       " 'since',\n",
       " 'show',\n",
       " 'countries',\n",
       " 'tv',\n",
       " 'workers',\n",
       " 'stop',\n",
       " 'without',\n",
       " 'change',\n",
       " 'where',\n",
       " 'work',\n",
       " 'him',\n",
       " 'foreign',\n",
       " 'during',\n",
       " 'fight',\n",
       " 'muslim',\n",
       " 'too',\n",
       " 'money',\n",
       " 'national',\n",
       " 'food',\n",
       " 'gets',\n",
       " 'secret',\n",
       " 'while',\n",
       " 'diary',\n",
       " 'days',\n",
       " 'family',\n",
       " 'four',\n",
       " 'even',\n",
       " 'own',\n",
       " 'accused',\n",
       " 'many',\n",
       " 'official',\n",
       " 'killing',\n",
       " 'long',\n",
       " 'states',\n",
       " 'prison',\n",
       " 'live',\n",
       " 'today',\n",
       " 'made',\n",
       " 'tax',\n",
       " 'any',\n",
       " '100',\n",
       " 'warns',\n",
       " 'canadian',\n",
       " 'kill',\n",
       " 'times',\n",
       " 'cut',\n",
       " 'david',\n",
       " 'least',\n",
       " 'r',\n",
       " 'gay',\n",
       " 'young',\n",
       " 'record',\n",
       " '20',\n",
       " 'parliament',\n",
       " 'shot',\n",
       " 'again',\n",
       " 'talks',\n",
       " 'don',\n",
       " 'soldiers',\n",
       " 'round',\n",
       " 'between',\n",
       " 'east',\n",
       " 'next',\n",
       " 'support',\n",
       " 'want',\n",
       " 're',\n",
       " 'peace',\n",
       " 'afghan',\n",
       " 'faces',\n",
       " 'snowden',\n",
       " 'aid',\n",
       " 'political',\n",
       " '6',\n",
       " 'second',\n",
       " 'told',\n",
       " 'move',\n",
       " 'head',\n",
       " 'election',\n",
       " 'terror',\n",
       " 'used',\n",
       " 'another',\n",
       " 'weapons',\n",
       " 'threat',\n",
       " 'she',\n",
       " '7',\n",
       " 'part',\n",
       " 'takes',\n",
       " 'border',\n",
       " 'rise',\n",
       " 'vote',\n",
       " 'open',\n",
       " 'test',\n",
       " 'shows',\n",
       " 'health',\n",
       " 'market',\n",
       " 'iranian',\n",
       " 'house',\n",
       " 'six',\n",
       " 'near',\n",
       " 'indian',\n",
       " 'wants',\n",
       " 'climate',\n",
       " 'online',\n",
       " 'rules',\n",
       " 'girl',\n",
       " 'return',\n",
       " 'rebels',\n",
       " 'jail',\n",
       " 'violence',\n",
       " '30',\n",
       " 'good',\n",
       " 'abuse',\n",
       " 'korean',\n",
       " 'force',\n",
       " 'left',\n",
       " 'sea',\n",
       " 'give',\n",
       " 'run',\n",
       " 'through',\n",
       " 'murder',\n",
       " 'bomb',\n",
       " 'action',\n",
       " 'trial',\n",
       " 'bid',\n",
       " 'pm',\n",
       " 'away',\n",
       " 'scientists',\n",
       " 'white',\n",
       " 'company',\n",
       " '11',\n",
       " 'nations',\n",
       " 'win',\n",
       " 'best',\n",
       " 'ready',\n",
       " 'including',\n",
       " 'much',\n",
       " 'real',\n",
       " 'system',\n",
       " 'suicide',\n",
       " 'history',\n",
       " 'hamas',\n",
       " 'months',\n",
       " 'mass',\n",
       " 'gas',\n",
       " 'google',\n",
       " 'land',\n",
       " 'control',\n",
       " 'illegal',\n",
       " 'nsa',\n",
       " 'palestinians',\n",
       " 'racing',\n",
       " '50',\n",
       " 'turkish',\n",
       " 'hits',\n",
       " 'future',\n",
       " 'charges',\n",
       " 'me',\n",
       " 'brazil',\n",
       " 'energy',\n",
       " 'largest',\n",
       " 'full',\n",
       " 'horse',\n",
       " 'going',\n",
       " '9',\n",
       " 'nato',\n",
       " 'forced',\n",
       " 'america',\n",
       " 'major',\n",
       " 'taliban',\n",
       " 'put',\n",
       " 'business',\n",
       " 'called',\n",
       " 'rape',\n",
       " 'great',\n",
       " 'drugs',\n",
       " 'despite',\n",
       " 'arabia',\n",
       " 'finds',\n",
       " 'church',\n",
       " 'labour',\n",
       " 'ever',\n",
       " 'press',\n",
       " 'john',\n",
       " '15',\n",
       " 'trade',\n",
       " 'biggest',\n",
       " 'makes',\n",
       " 'economy',\n",
       " 'black',\n",
       " 'need',\n",
       " 'see',\n",
       " 'pope',\n",
       " 'final',\n",
       " 'data',\n",
       " 'ireland',\n",
       " 'around',\n",
       " 'legal',\n",
       " 'across',\n",
       " 'v',\n",
       " 'become',\n",
       " 'lost',\n",
       " 'bush',\n",
       " '8',\n",
       " 'arsenal',\n",
       " 'hundreds',\n",
       " 'greece',\n",
       " 'town',\n",
       " 'kills',\n",
       " 'month',\n",
       " 'leave',\n",
       " 'theatre',\n",
       " 'intelligence',\n",
       " 'banned',\n",
       " 'row',\n",
       " 'spain',\n",
       " 'using',\n",
       " 'keep',\n",
       " '12',\n",
       " 'banks',\n",
       " 'find',\n",
       " 'students',\n",
       " 'laws',\n",
       " 'leaders',\n",
       " 'battle',\n",
       " 'crime',\n",
       " 'study',\n",
       " 'price',\n",
       " 'goes',\n",
       " 'campaign',\n",
       " 'crimes',\n",
       " 'euro',\n",
       " 'strikes',\n",
       " 'inquiry',\n",
       " 'japanese',\n",
       " 'evidence',\n",
       " 'job',\n",
       " 'reports',\n",
       " 'did',\n",
       " 'tells',\n",
       " 'arab',\n",
       " 'university',\n",
       " 'african',\n",
       " 'egyptian',\n",
       " 'policy',\n",
       " 'wins',\n",
       " 'chelsea',\n",
       " 'fears',\n",
       " 'every',\n",
       " 'turn',\n",
       " 'according',\n",
       " 'brown',\n",
       " 'those',\n",
       " 'game',\n",
       " 'same',\n",
       " 'liverpool',\n",
       " 'warning',\n",
       " 'behind',\n",
       " 'opposition',\n",
       " 'does',\n",
       " 'northern',\n",
       " 'interview',\n",
       " 'know',\n",
       " 'half',\n",
       " 'spy',\n",
       " 'won',\n",
       " 'middle',\n",
       " 'jewish',\n",
       " 'cia',\n",
       " 'red',\n",
       " 'third',\n",
       " 'far',\n",
       " 'arms',\n",
       " 'held',\n",
       " 'past',\n",
       " 'libya',\n",
       " 'scandal',\n",
       " 'claim',\n",
       " 'victims',\n",
       " 'missing',\n",
       " 'torture',\n",
       " 'citizens',\n",
       " 'authorities',\n",
       " 'car',\n",
       " 'here',\n",
       " 'also',\n",
       " 'capital',\n",
       " 'let',\n",
       " 'jailed',\n",
       " 'close',\n",
       " 'ship',\n",
       " 'threatens',\n",
       " 'dies',\n",
       " 'film',\n",
       " 'then',\n",
       " 'muslims',\n",
       " 'assange',\n",
       " 'mexican',\n",
       " 'well',\n",
       " 'key',\n",
       " 'bans',\n",
       " 'arrest',\n",
       " 'western',\n",
       " 'surveillance',\n",
       " 'corruption',\n",
       " 'moscow',\n",
       " 'italy',\n",
       " 'admits',\n",
       " 'never',\n",
       " 'king',\n",
       " 'nation',\n",
       " 'save',\n",
       " 'role',\n",
       " 'died',\n",
       " 'making',\n",
       " 'activists',\n",
       " 'fear',\n",
       " 'story',\n",
       " 'join',\n",
       " 'phone',\n",
       " 'general',\n",
       " 'hours',\n",
       " 'manchester',\n",
       " 'nearly',\n",
       " 'order',\n",
       " 'agency',\n",
       " 'come',\n",
       " 'dutch',\n",
       " 'tuesday',\n",
       " 'cuts',\n",
       " 'start',\n",
       " 'financial',\n",
       " 'ahead',\n",
       " 'schools',\n",
       " 'plane',\n",
       " '40',\n",
       " 'royal',\n",
       " 'guardian',\n",
       " 'service',\n",
       " 'plant',\n",
       " 'irish',\n",
       " 'watch',\n",
       " 'green',\n",
       " 'cost',\n",
       " 'fukushima',\n",
       " 'social',\n",
       " 'radio',\n",
       " 'student',\n",
       " 'companies',\n",
       " 'iraqi',\n",
       " 'place',\n",
       " 'leeds',\n",
       " 'cash',\n",
       " 'post',\n",
       " 'girls',\n",
       " '14',\n",
       " 'trying',\n",
       " 'taking',\n",
       " 'sweden',\n",
       " 'investigation',\n",
       " 'economic',\n",
       " 'local',\n",
       " 'look',\n",
       " 'letter',\n",
       " 'number',\n",
       " 'kim',\n",
       " 'members',\n",
       " 'almost',\n",
       " 'earthquake',\n",
       " 'monday',\n",
       " 'body',\n",
       " 'building',\n",
       " 'female',\n",
       " 'die',\n",
       " 'private',\n",
       " 'bad',\n",
       " 'night',\n",
       " 'judge',\n",
       " 'millions',\n",
       " 'council',\n",
       " 'freedom',\n",
       " 'zealand',\n",
       " 'given',\n",
       " 'think',\n",
       " 'race',\n",
       " 'access',\n",
       " 'pick',\n",
       " 'groups',\n",
       " 'play',\n",
       " 'ex',\n",
       " 'democracy',\n",
       " 'charged',\n",
       " 'sign',\n",
       " 'likely',\n",
       " 'population',\n",
       " 'rate',\n",
       " 'staff',\n",
       " 'among',\n",
       " 'street',\n",
       " 'working',\n",
       " 'sentenced',\n",
       " 'speech',\n",
       " 'music',\n",
       " 'act',\n",
       " 'demand',\n",
       " 'paris',\n",
       " 'growing',\n",
       " 'puts',\n",
       " 'greek',\n",
       " 'guilty',\n",
       " 'prize',\n",
       " 'terrorist',\n",
       " 'percent',\n",
       " 'fa',\n",
       " 'debate',\n",
       " 'office',\n",
       " 'journalists',\n",
       " 'team',\n",
       " 'earth',\n",
       " 'poor',\n",
       " 'mps',\n",
       " 'lead',\n",
       " 'worst',\n",
       " 'gives',\n",
       " 'homes',\n",
       " 'seen',\n",
       " 'ago',\n",
       " 'missile',\n",
       " 'break',\n",
       " 'documents',\n",
       " 'facebook',\n",
       " 'released',\n",
       " 'english',\n",
       " 'taken',\n",
       " '500',\n",
       " 'civilians',\n",
       " 'pirate',\n",
       " 'site',\n",
       " '18',\n",
       " 'massive',\n",
       " 'pro',\n",
       " 'firm',\n",
       " 'debt',\n",
       " '16',\n",
       " 'italian',\n",
       " 'pop',\n",
       " 'central',\n",
       " 'risk',\n",
       " 'charge',\n",
       " 'pakistani',\n",
       " 'lives',\n",
       " '25',\n",
       " 'list',\n",
       " 'line',\n",
       " 'asylum',\n",
       " 'reveals',\n",
       " 'reported',\n",
       " 'm',\n",
       " 'web',\n",
       " 'boy',\n",
       " 'deaths',\n",
       " 'very',\n",
       " 'brief',\n",
       " 'civil',\n",
       " 'pressure',\n",
       " 'amid',\n",
       " 'fall',\n",
       " 'living',\n",
       " 'march',\n",
       " 'simon',\n",
       " 'religious',\n",
       " 'sanctions',\n",
       " 'hospital',\n",
       " 'fighting',\n",
       " 'wednesday',\n",
       " 'streets',\n",
       " 'little',\n",
       " 'marriage',\n",
       " 'research',\n",
       " 'saying',\n",
       " 'tony',\n",
       " 'non',\n",
       " 'huge',\n",
       " 'inside',\n",
       " 'build',\n",
       " 'space',\n",
       " 'within',\n",
       " '13',\n",
       " 'stay',\n",
       " 'protect',\n",
       " 'age',\n",
       " 'suspected',\n",
       " 'low',\n",
       " 'victory',\n",
       " 'conflict',\n",
       " 'latest',\n",
       " 'due',\n",
       " 'raid',\n",
       " 'information',\n",
       " 'journalist',\n",
       " 'games',\n",
       " 'less',\n",
       " 'thursday',\n",
       " 'spanish',\n",
       " 'scotland',\n",
       " '300',\n",
       " 'justice',\n",
       " 'catholic',\n",
       " 'such',\n",
       " 'tennis',\n",
       " 'regime',\n",
       " 'launch',\n",
       " 'cancer',\n",
       " 'doctors',\n",
       " 'parents',\n",
       " 'ebola',\n",
       " 'orders',\n",
       " 'com',\n",
       " 'murdoch',\n",
       " 'longer',\n",
       " 'newcastle',\n",
       " 'hold',\n",
       " 'ordered',\n",
       " 'officers',\n",
       " 'gold',\n",
       " 'better',\n",
       " 'crash',\n",
       " 'jobs',\n",
       " 'terrorism',\n",
       " 'sexual',\n",
       " 'cameron',\n",
       " 'son',\n",
       " 'love',\n",
       " 'officer',\n",
       " 'these',\n",
       " 'norway',\n",
       " 'following',\n",
       " 'richard',\n",
       " 'marijuana',\n",
       " 'coast',\n",
       " 'militants',\n",
       " 'islam',\n",
       " 'poll',\n",
       " 'revealed',\n",
       " 'massacre',\n",
       " 'drone',\n",
       " 'vatican',\n",
       " 'hard',\n",
       " 'embassy',\n",
       " 'rich',\n",
       " 'name',\n",
       " 'art',\n",
       " 'prices',\n",
       " 'education',\n",
       " 'criminal',\n",
       " 'industry',\n",
       " 'seven',\n",
       " 'warned',\n",
       " 'ruling',\n",
       " 'disaster',\n",
       " 'solar',\n",
       " 'turns',\n",
       " 'appeal',\n",
       " 'enough',\n",
       " 'o',\n",
       " 'swedish',\n",
       " 'led',\n",
       " 'leaves',\n",
       " 'getting',\n",
       " 'attacked',\n",
       " 'response',\n",
       " 'jews',\n",
       " 'caught',\n",
       " 'wrong',\n",
       " 'step',\n",
       " 'running',\n",
       " 'williams',\n",
       " 'zimbabwe',\n",
       " 'michael',\n",
       " '24',\n",
       " 'alleged',\n",
       " 'block',\n",
       " 'airport',\n",
       " 'champions',\n",
       " 'venezuela',\n",
       " 'injured',\n",
       " 'light',\n",
       " 'allow',\n",
       " 'hand',\n",
       " 'worth',\n",
       " 'bring',\n",
       " 'sales',\n",
       " 'mother',\n",
       " '200',\n",
       " 'per',\n",
       " 'spying',\n",
       " 'search',\n",
       " 'deadly',\n",
       " 'discovered',\n",
       " 'rejects',\n",
       " 'sent',\n",
       " 'outside',\n",
       " 'raped',\n",
       " 'boss',\n",
       " 'victim',\n",
       " 'stand',\n",
       " 'george',\n",
       " 'jerusalem',\n",
       " 'senior',\n",
       " 'early',\n",
       " 'gang',\n",
       " 'small',\n",
       " '17',\n",
       " 'julian',\n",
       " 'ask',\n",
       " 'edward',\n",
       " 'iceland',\n",
       " 'defense',\n",
       " 'road',\n",
       " 'founder',\n",
       " 'reveal',\n",
       " 'failed',\n",
       " 'ground',\n",
       " 'loses',\n",
       " 'release',\n",
       " 'urges',\n",
       " 'village',\n",
       " 'class',\n",
       " 'launches',\n",
       " 'nigeria',\n",
       " 'revolution',\n",
       " 'beijing',\n",
       " 'services',\n",
       " 'bay',\n",
       " 'member',\n",
       " 'sale',\n",
       " 'giant',\n",
       " 'comes',\n",
       " 'once',\n",
       " 'seeks',\n",
       " 'decision',\n",
       " 've',\n",
       " 'square',\n",
       " 'defence',\n",
       " 'fuel',\n",
       " 'users',\n",
       " 'armed',\n",
       " 'festival',\n",
       " 'classical',\n",
       " 'possible',\n",
       " 'weeks',\n",
       " 'care',\n",
       " 'hope',\n",
       " 'sunday',\n",
       " 'christian',\n",
       " 'ukrainian',\n",
       " 'soldier',\n",
       " 'libyan',\n",
       " 'd',\n",
       " 'lose',\n",
       " 'activist',\n",
       " 'needs',\n",
       " 'problem',\n",
       " 'refugees',\n",
       " 'book',\n",
       " 'offers',\n",
       " 'newspaper',\n",
       " 'others',\n",
       " 'looks',\n",
       " 'offer',\n",
       " 'net',\n",
       " 'players',\n",
       " 'special',\n",
       " 'father',\n",
       " 'large',\n",
       " 'demands',\n",
       " 'level',\n",
       " 'premiership',\n",
       " 'fraud',\n",
       " 'target',\n",
       " 'double',\n",
       " 'park',\n",
       " 'boost',\n",
       " 'medical',\n",
       " 'buy',\n",
       " '60',\n",
       " 'region',\n",
       " 'collapse',\n",
       " 'denies',\n",
       " 'really',\n",
       " 'send',\n",
       " 'baby',\n",
       " 'front',\n",
       " 'nothing',\n",
       " 'island',\n",
       " 'leading',\n",
       " 'self',\n",
       " 'worlds',\n",
       " 'govt',\n",
       " 'bin',\n",
       " 'got',\n",
       " 'targets',\n",
       " 'few',\n",
       " 'science',\n",
       " 'elections',\n",
       " 'jong',\n",
       " 'flight',\n",
       " 'hopes',\n",
       " 'visit',\n",
       " 'calling',\n",
       " 'assad',\n",
       " 'thought',\n",
       " 'hall',\n",
       " 'supreme',\n",
       " 'fund',\n",
       " 'christmas',\n",
       " 'century',\n",
       " 'yet',\n",
       " 'backs',\n",
       " 'emergency',\n",
       " 'friday',\n",
       " 'train',\n",
       " 'mark',\n",
       " 'attempt',\n",
       " 'paid',\n",
       " 'ends',\n",
       " 'personal',\n",
       " 'recent',\n",
       " 'vladimir',\n",
       " ...]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veere\\.conda\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.syn0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veere\\.conda\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\veere\\.conda\\envs\\py37\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'chocolate'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"vanilla chocolate cinnamon dish\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fish', 0.9806402921676636),\n",
       " ('swede', 0.9802969694137573),\n",
       " ('reading', 0.9795242547988892),\n",
       " ('shark', 0.9787427186965942),\n",
       " ('horses', 0.9776006937026978),\n",
       " ('2005', 0.977145791053772),\n",
       " ('scream', 0.977135956287384),\n",
       " ('windows', 0.9769838452339172),\n",
       " ('coral', 0.9768654704093933),\n",
       " ('warwickshire', 0.9767245054244995)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x28028b8bd68>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    Convert a review to a list of words. Removal of stop words is optional.\n",
    "    \"\"\"\n",
    "    # remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "    \n",
    "    # convert to lower case and split at whitespace\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    # remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veere\\.conda\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.09716212,  0.25308648, -0.81622803,  0.12601708, -0.31018546,\n",
       "       -0.64120585, -0.6678692 , -0.39942732,  0.92052054, -0.01599647,\n",
       "       -1.25072   , -1.1435418 , -0.6071113 , -0.22742519,  0.93730783,\n",
       "        1.5233895 ,  0.87616867, -0.1634026 ,  0.83134747, -0.5810564 ,\n",
       "       -1.8829064 , -1.4458685 ,  0.24974418,  0.7925907 , -0.92711776,\n",
       "        1.5666753 ,  0.34322417, -1.2988342 , -1.4877853 , -0.19611017,\n",
       "       -0.68473667,  0.15134926, -0.9546291 , -1.4013103 ,  0.7772082 ,\n",
       "        1.3046649 ,  0.0547561 ,  1.2114898 ,  0.30105883,  0.12841275,\n",
       "       -0.3900131 , -0.3688361 ,  0.543926  , -0.683944  ,  0.07929496,\n",
       "        0.62629485, -1.4823004 ,  0.419769  ,  0.71639174, -1.2040406 ,\n",
       "       -1.5398651 , -0.99164885,  0.613096  ,  0.5822072 ,  0.6084894 ,\n",
       "        1.5841928 , -0.8538848 , -0.4669466 ,  1.4928977 , -0.03336158,\n",
       "       -1.1252936 ,  0.34615043,  0.8581092 ,  0.08103255, -1.0325605 ,\n",
       "       -0.03444998,  1.0013747 ,  0.16876529,  0.03282152,  0.21622357,\n",
       "       -0.0101372 ,  1.1451341 , -0.73490006,  0.03030931,  0.31443864,\n",
       "        1.6007608 ,  0.19560455, -0.8233625 , -0.74074507,  0.33409923,\n",
       "        1.0116935 ,  0.43273982, -0.80264074, -1.5252812 ,  0.25326803,\n",
       "        1.624792  ,  1.1389267 ,  0.91170096,  0.48695236, -0.96214956,\n",
       "       -0.30740866, -1.1316457 , -0.0474609 , -0.21811607,  0.7387499 ,\n",
       "        1.1696391 ,  0.01292698, -0.9049354 ,  0.65378916, -1.2347233 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Average the word vectors for a set of words\n",
    "    \"\"\"\n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index2word)  # words known to the model\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec,model[word])\n",
    "    \n",
    "    feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_avg_feature_vecs(reviews, model, num_features):\n",
    "    \"\"\"\n",
    "    Calculate average feature vectors for all reviews\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    review_feature_vecs = np.zeros((len(reviews),num_features), dtype='float32')  # pre-initialize (for speed)\n",
    "    \n",
    "    for review in reviews:\n",
    "#         print(review,'review')\n",
    "#         print(counter,'counter')\n",
    "        review_feature_vecs[counter] = make_feature_vec(review, model, num_features)\n",
    "#         print(counter,'counter')\n",
    "        counter = counter + 1\n",
    "    return review_feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "(2,)\n",
      "[2. 3.]\n",
      "(2,)\n",
      "[4. 6.]\n",
      "(2,)\n",
      "[ 9. 12.]\n",
      "(2,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3., 4.])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x= np.zeros((2,))\n",
    "# print(x)\n",
    "# print(x.shape)\n",
    "# x = np.add(x, [2,3])\n",
    "# print(x)\n",
    "# print(x.shape)\n",
    "# x = np.add(x, [2,3])\n",
    "# print(x)\n",
    "# print(x.shape)\n",
    "# x = np.add(x, [5,6])\n",
    "# print(x)\n",
    "# print(x.shape)\n",
    "# x=np.divide(x, 3)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veere\\.conda\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# calculate average feature vectors for training and test sets\n",
    "clean_train_reviews = []\n",
    "for review in headlines:\n",
    "    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "trainDataVecs = get_avg_feature_vecs(clean_train_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veere\\.conda\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "clean_test_reviews = []\n",
    "for review in test_transform:\n",
    "    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "testDataVecs = get_avg_feature_vecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(trainDataVecs, train['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1879\n",
      "           1       1.00      1.00      1.00      2096\n",
      "\n",
      "    accuracy                           1.00      3975\n",
      "   macro avg       1.00      1.00      1.00      3975\n",
      "weighted avg       1.00      1.00      1.00      3975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = forest.predict(trainDataVecs)\n",
    "print(classification_report(train['Label'], result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82       186\n",
      "           1       0.81      0.85      0.83       192\n",
      "\n",
      "    accuracy                           0.83       378\n",
      "   macro avg       0.83      0.82      0.83       378\n",
      "weighted avg       0.83      0.83      0.83       378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = forest.predict(testDataVecs)\n",
    "print(classification_report(test['Label'], result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = forest.predict_proba(testDataVecs)[:, 1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(test['Label'], probs)\n",
    "auc = roc_auc_score(test['Label'], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='AUC {:.3f}'.format(auc))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train.drop(['Date'], axis=1)\n",
    "test_x = test.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data(x):\n",
    "    try:\n",
    "        if 'text' in x.index:\n",
    "            x.drop(['text'], inplace=True)\n",
    "        if 'Label' in x.index:\n",
    "            x.drop(['Label'], inplace=True)\n",
    "        return ' '.join(['' if pd.isna(i) else i for i in x ])\n",
    "    except:\n",
    "        print(x)\n",
    "        e\n",
    "train_x['text'] = train_x.apply(join_data, axis=1)\n",
    "test_x['text'] = test_x.apply(join_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3723    1\n",
       "3724    0\n",
       "3725    0\n",
       "3726    1\n",
       "3727    1\n",
       "       ..\n",
       "4096    0\n",
       "4097    1\n",
       "4098    1\n",
       "4099    1\n",
       "4100    1\n",
       "Name: Label, Length: 378, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    train_x.text.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=128, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    test_x.text.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=128, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(train_x.Label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(test_x.Label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3975"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1037,  1005, 17666, 21621,  2000,  3136,  1005,  1024, 27059,\n",
       "         2013,  1996, 15748,  4311,  3556, 11522,  8099,  1005,  7107,  2718,\n",
       "        20934, 27153,  5132,  2990,  4152,  2010, 17260,  2015,  2006,  2012,\n",
       "         3256,  1011,  3147,  4074,  8488,  2004, 13955, 28621, 16473,  2039,\n",
       "         2005,  2142, 22595, 11258,  3653,  3567,  4014,  2004,  9899, 27594,\n",
       "         2015, 18022,  1005,  1055,  2283,  7501, 18205,  3168,  4138,  8130,\n",
       "         2015, 29000,  2061,  2898,  1997,  2019,  3733,  4539,  7350,  5333,\n",
       "         1037,  3221,  2000,  2358, 21531,  2906,  1005,  1055,  2834,  3313,\n",
       "         2148,  5867,  9326,  1010,  7873,  3477,  1996,  6531,  8691,  2015,\n",
       "         2192, 23698,  1037, 22446, 10800,  6586,  2283,  2066,  2009,  1005,\n",
       "         1055,  2639,  4929,  8588,  2031,  2357,  2046, 12559,  2015, 22475,\n",
       "         3505, 11269,  3231,  2100, 29124,  1005,  1055, 19982,  3372, 21395,\n",
       "         2099, 12976,  2000,  2718, 16785, 13493,  7245,   102])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labels_train[3])\n",
    "labels_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'0':0,'1':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 4\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "# del variables\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6727e454b194a559c83a585ee967901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 0.6961748629988319\n",
      "Validation loss: 0.6920095838998493\n",
      "F1 Score (Weighted): 0.45323495827843646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.6960317072734027\n",
      "Validation loss: 0.6996623729404651\n",
      "F1 Score (Weighted): 0.3421888053467001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.6863969121900363\n",
      "Validation loss: 0.6407739902797498\n",
      "F1 Score (Weighted): 0.6035107376283847\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 0.5822818887155781\n",
      "Validation loss: 0.4984924586195695\n",
      "F1 Score (Weighted): 0.7669348928873805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 0.42420668632234304\n",
      "Validation loss: 0.5988056191684384\n",
      "F1 Score (Weighted): 0.7988403824469398\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6\n",
      "Training loss: 0.3226003704668244\n",
      "Validation loss: 0.8390182853933718\n",
      "F1 Score (Weighted): 0.8227426362197252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7\n",
      "Training loss: 0.21635653099921318\n",
      "Validation loss: 1.0750525702247518\n",
      "F1 Score (Weighted): 0.8253968253968254\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8\n",
      "Training loss: 0.13248309149109722\n",
      "Validation loss: 1.2075060941767572\n",
      "F1 Score (Weighted): 0.8253479402232099\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9\n",
      "Training loss: 0.09679155796830696\n",
      "Validation loss: 1.2319806703597638\n",
      "F1 Score (Weighted): 0.8333391656110244\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10\n",
      "Training loss: 0.059907455344841536\n",
      "Validation loss: 1.2633818488662272\n",
      "F1 Score (Weighted): 0.8412520663360529\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "device = 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('finetuned_BERT_epoch_10.model', map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0\n",
      "Accuracy: 160/186\n",
      "\n",
      "Class: 1\n",
      "Accuracy: 158/192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "\n",
    "accuracy_per_class(predictions, true_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0\n",
      "Accuracy: 1876/1879\n",
      "\n",
      "Class: 1\n",
      "Accuracy: 2092/2096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_train)\n",
    "accuracy_per_class(predictions, true_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
